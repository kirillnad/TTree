## Семантический поиск (по смыслу)

В Memus реализован “поиск по смыслу” для блоков на базе:
- PostgreSQL + расширение `pgvector` (хранение векторов и KNN);
- embeddings через OpenAI API.

Также есть “RAG-страница” (UID `RAG`) — это **read-only** представление результатов AI‑поиска, где сверху генерируется краткая **сводка** по найденным блокам.

### Что именно ищется

- Индексируются **блоки** (текст блока + контекст заголовка статьи).
- Результат поиска — **похожие блоки** (не генерация ответа).
  - Для embeddings берётся строка вида: `"<title статьи>\\n<plain text блока>"`, где plain text получается из HTML блока через `strip_html(...)`.
  - Если этот текст длиннее `SERVPY_EMBEDDING_MAX_CHARS`, он **не обрезается**, а разбивается на чанки. Embeddings считаются для каждого чанка, а итоговый embedding блока — это **усреднение** по чанкам (с L2-нормализацией), чтобы получить один вектор на блок.

### Требования

1) PostgreSQL с установленным расширением `pgvector`.
   - В БД должно выполниться `CREATE EXTENSION vector;`
   - Схема пытается создать это расширение автоматически при старте (`init_schema()`), но если у пользователя БД нет прав — семантический поиск будет недоступен.

2) Доступный провайдер embeddings (OpenAI).

### Переменные окружения

- `SERVPY_DATABASE_URL` — DSN PostgreSQL (обязателен).
- `SERVPY_OPENAI_API_KEY` / `OPENAI_API_KEY` — API key для OpenAI (обязателен).
- `SERVPY_OPENAI_EMBED_MODEL` — модель embeddings OpenAI, по умолчанию `text-embedding-3-small`.
- `SERVPY_EMBEDDING_DIM` — размерность эмбеддинга, по умолчанию `768`.
- `SERVPY_EMBEDDING_MAX_CHARS` — максимальная длина одного чанка (по умолчанию `12000`); длинный текст блока разбивается на чанки и агрегируется в один embedding.
- `SERVPY_SEMANTIC_REINDEX_CONCURRENCY` — параллелизм переиндексации (потоки), по умолчанию `4`.
- `SERVPY_SEMANTIC_REINDEX_BLOCK_BATCH_SIZE` — сколько блоков обрабатывается одним батч‑заданием reindex (по умолчанию `32`).
- `SERVPY_EMBEDDINGS_API_BATCH_SIZE` — сколько текстов отправляется в один запрос `POST /v1/embeddings` (по умолчанию `64`).

RAG (сводка):
- `SERVPY_RAG_SUMMARY_MODEL` — модель OpenAI для генерации сводки (по умолчанию `gpt-4o-mini`; можно поставить `gpt-4.1-mini`).
- `SERVPY_RAG_SUMMARY_TIMEOUT_SECONDS` — таймаут запроса сводки (по умолчанию `60`).
- `SERVPY_RAG_SUMMARY_MAX_BLOCKS` — максимум блоков из выдачи, которые попадут в контекст сводки (по умолчанию `40`).
- `SERVPY_RAG_SUMMARY_MAX_TOTAL_CHARS` — общий лимит символов контекста сводки (по умолчанию `24000`).
- `SERVPY_RAG_SUMMARY_MAX_BLOCK_CHARS` — лимит символов на один блок в контексте сводки (по умолчанию `2000`).

Важно: размерность `SERVPY_EMBEDDING_DIM` должна совпадать с размерностью модели embeddings и с типом в БД `vector(768)` в таблице `block_embeddings`.

### Схема в БД

Создаётся таблица:
- `block_embeddings(block_id PRIMARY KEY, author_id, article_id, article_title, plain_text, embedding vector(768), updated_at)`

Если доступен индексный метод `hnsw` (зависит от версии pgvector), создаётся индекс:
- `idx_block_embeddings_embedding_hnsw ON block_embeddings USING hnsw (embedding vector_cosine_ops)`

### API

- `GET /api/search/semantic?q=...`
  - Возвращает список похожих блоков (topK, по умолчанию 30), включая `score`.

- `POST /api/search/semantic/reindex`
  - Стартует (или “подхватывает” уже запущенную) **фоновую** переиндексацию embeddings для текущего пользователя.
  - Поддерживает режимы (body JSON):
    - `{ "mode": "all" }` — пересчитать всё заново;
    - `{ "mode": "missing" }` — обработать только блоки, которых нет в `block_embeddings` (быстрее).
  - Возвращает объект задачи (`status: running|completed|failed|cooldown|cancelled`, прогресс `processed/total`, счётчики `indexed/failed`).
  - Задача продолжает выполняться даже если HTTP‑запрос оборвётся/зависнет у клиента.

- `GET /api/search/semantic/reindex/status`
  - Возвращает текущий статус/прогресс задачи переиндексации для пользователя, либо `{ "status": "idle" }`.

- `POST /api/search/semantic/reindex/cancel`
  - Запрашивает отмену текущей задачи (если она запущена) и возвращает её объект.
  - Отмена “мягкая”: задача остановится на ближайшей итерации.

- `POST /api/search/semantic/rag-summary`
  - Генерирует сводку по результатам AI‑поиска.
  - Вход: `{ "query": "...", "results": [ ... ] }` (обычно это выдача `GET /api/search/semantic`).
  - Выход: `{ "summaryHtml": "<p>...</p><ul>...</ul>" }` — **только HTML**, без Markdown.

Переиндексацию можно запускать не чаще, чем раз в ~10 минут: при слишком частом запуске API вернёт `status: cooldown`.

### UI → переключение режима поиска

В панели `search-panel` (в сайдбаре) справа теперь есть кнопка c иконкой `Ai`. Она переключает режим между:
  - “Классическим” поиском (`/api/search`)
  - “Семантическим” (`/api/search/semantic`).
При переключении:
  - меняется placeholder;
  - опыт сохраняется на клиенте, следующий ввод сразу делает запрос в выбранном режиме;
  - переключатель подсвечивается, когда активен семантический режим.

### Как заполнить индекс

1) Убедиться, что `pgvector` доступен (иначе endpoint будет отвечать 503).
2) Убедиться, что настроен OpenAI (ключ и модель), и что размерность совпадает с `SERVPY_EMBEDDING_DIM` и таблицей `block_embeddings`.
3) Зайти в Memus под пользователем и вызвать `POST /api/search/semantic/reindex` (или меню пользователя → “Переиндексировать поиск”).
4) Проверять прогресс через `GET /api/search/semantic/reindex/status` (в UI прогресс показывается тостом).

### Автообновление

Индекс обновляется автоматически при:
- сохранении текста блока (`update_block`),
- вставке блока с готовым контентом (payload),
- удалении блоков (удаляются и embeddings),
- переносе блока в другую статью (переиндексация переносимого поддерева).

## RAG-страница (UID `RAG`)

RAG в Memus — это не “ответ на вопрос” в виде чата, а удобная страница‑сборник:
- отдельный блок “Сводка” (генерация через OpenAI),
- затем блок “AI‑поиск: результаты” (мета-информация),
- затем каждый найденный блок отдельным блоком (read-only).

### Когда и как она строится

- Кнопка `R` рядом с `Ai` появляется только когда:
  - включён AI‑режим (`Ai` активен),
  - введён запрос,
  - есть результаты поиска.
- По нажатию `R` сохраняется “снимок” выдачи и открывается `/article/RAG`.
- Содержимое RAG сохраняется до следующего нажатия `R` (не очищается при переходах).
- Клик по блоку на странице RAG открывает исходную статью и скроллит/раскрывает ветку так, чтобы блок был видим.

### Как формируется сводка (prompt)

Сервер берёт `query` и результаты поиска и формирует контекст:
- каждый блок приводится к plain text (через `strip_html`),
- берутся топ‑N блоков (`SERVPY_RAG_SUMMARY_MAX_BLOCKS`),
- каждый блок ограничивается по длине (`SERVPY_RAG_SUMMARY_MAX_BLOCK_CHARS`),
- общий контекст ограничивается (`SERVPY_RAG_SUMMARY_MAX_TOTAL_CHARS`).

После списка фрагментов сервер добавляет “инструкции” для модели:
- вернуть **только HTML** (без Markdown),
- язык: русский,
- формат: 1–2 предложения + `<ul>` с 5–12 пунктами,
- отметить противоречия отдельным пунктом, если они есть.
